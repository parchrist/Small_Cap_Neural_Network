{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9de32038",
   "metadata": {},
   "source": [
    "# LTSM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d24c12c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enviorment Setup\n",
    "import numpy as np\n",
    "import keras\n",
    "import os \n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2801bb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    data = []\n",
    "    required_columns = ['open', 'high', 'low', 'close', 'volume', 'number_of_trades']\n",
    "    for filename in os.listdir(path):\n",
    "        file_path = os.path.join(path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        if set(required_columns).issubset(df.columns):\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y-%m-%d')\n",
    "            df['time'] = pd.to_datetime(df['time'], format='%H:%M:%S').dt.time\n",
    "            df['timestamp'] = df.apply(lambda x: int(x['timestamp'].timestamp() * 1000), axis=1)\n",
    "            df = df.drop(columns=['time'])\n",
    "            df = df[required_columns + ['timestamp']]\n",
    "            data.append(df)\n",
    "    return pd.concat(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c2885b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    return (data - np.mean(data, axis=0)) / np.std(data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ab042490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data, time_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps - 1):\n",
    "        X.append(data[i:(i + time_steps), :-1])\n",
    "        y.append(data[i + time_steps, -1])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "92d4f07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, time_steps, neurons):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons, input_shape=(time_steps, input_shape[2]), return_sequences=True))\n",
    "    model.add(LSTM(neurons))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6dbc2e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the csv Data\n",
    "\n",
    "data = load_data('F:/Small_Cap_LTSM/Ticker_Information/Neural_Network_Training/Training_data')\n",
    "data = normalize(data.values)\n",
    "time_steps = 30\n",
    "X, y = create_dataset(data, time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd18efa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "19441/19441 [==============================] - 385s 20ms/step - loss: 0.8017\n",
      "Epoch 2/50\n",
      "19441/19441 [==============================] - 401s 21ms/step - loss: 0.7111\n",
      "Epoch 3/50\n",
      "19441/19441 [==============================] - 413s 21ms/step - loss: 0.6439\n",
      "Epoch 4/50\n",
      "19441/19441 [==============================] - 428s 22ms/step - loss: 0.5815\n",
      "Epoch 5/50\n",
      "19441/19441 [==============================] - 449s 23ms/step - loss: 0.5233\n",
      "Epoch 6/50\n",
      "19441/19441 [==============================] - 490s 25ms/step - loss: 0.4683\n",
      "Epoch 7/50\n",
      "19441/19441 [==============================] - 523s 27ms/step - loss: 0.4212\n",
      "Epoch 8/50\n",
      "19441/19441 [==============================] - 567s 29ms/step - loss: 0.3793\n",
      "Epoch 9/50\n",
      "19441/19441 [==============================] - 606s 31ms/step - loss: 0.3435\n",
      "Epoch 10/50\n",
      "19441/19441 [==============================] - 638s 33ms/step - loss: 0.3130\n",
      "Epoch 11/50\n",
      "19441/19441 [==============================] - 668s 34ms/step - loss: 0.2864\n",
      "Epoch 12/50\n",
      "19441/19441 [==============================] - 699s 36ms/step - loss: 0.2641\n",
      "Epoch 13/50\n",
      "19441/19441 [==============================] - 736s 38ms/step - loss: 0.2447\n",
      "Epoch 14/50\n",
      "19441/19441 [==============================] - 784s 40ms/step - loss: 0.2287\n",
      "Epoch 15/50\n",
      "19441/19441 [==============================] - 814s 42ms/step - loss: 0.2148\n",
      "Epoch 16/50\n",
      "19441/19441 [==============================] - 855s 44ms/step - loss: 0.2013\n",
      "Epoch 17/50\n",
      "19441/19441 [==============================] - 887s 46ms/step - loss: 0.1894\n",
      "Epoch 18/50\n",
      "19441/19441 [==============================] - 936s 48ms/step - loss: 0.1801\n",
      "Epoch 19/50\n",
      "19441/19441 [==============================] - 978s 50ms/step - loss: 0.1725\n",
      "Epoch 20/50\n",
      "19441/19441 [==============================] - 976s 50ms/step - loss: 0.1647\n",
      "Epoch 21/50\n",
      "19441/19441 [==============================] - 1025s 53ms/step - loss: 0.1580\n",
      "Epoch 22/50\n",
      "19441/19441 [==============================] - 1076s 55ms/step - loss: 0.1523\n",
      "Epoch 23/50\n",
      "19441/19441 [==============================] - 1124s 58ms/step - loss: 0.1467\n",
      "Epoch 24/50\n",
      "19441/19441 [==============================] - 1169s 60ms/step - loss: 0.1418\n",
      "Epoch 25/50\n",
      "19441/19441 [==============================] - 1229s 63ms/step - loss: 0.1376\n",
      "Epoch 26/50\n",
      "19441/19441 [==============================] - 1270s 65ms/step - loss: 0.1339\n",
      "Epoch 27/50\n",
      "19441/19441 [==============================] - 1321s 68ms/step - loss: 0.1295\n",
      "Epoch 28/50\n",
      "19441/19441 [==============================] - 1383s 71ms/step - loss: 0.1276\n",
      "Epoch 29/50\n",
      "19441/19441 [==============================] - 1438s 74ms/step - loss: 0.1237\n",
      "Epoch 30/50\n",
      "19441/19441 [==============================] - 1496s 77ms/step - loss: 0.1206\n",
      "Epoch 31/50\n",
      "19441/19441 [==============================] - 1557s 80ms/step - loss: 0.1200\n",
      "Epoch 32/50\n",
      "19441/19441 [==============================] - 1643s 85ms/step - loss: 0.1160\n",
      "Epoch 33/50\n",
      "19441/19441 [==============================] - 1670s 86ms/step - loss: 0.1128\n",
      "Epoch 34/50\n",
      "19441/19441 [==============================] - 1724s 89ms/step - loss: 0.1108\n",
      "Epoch 35/50\n",
      "19441/19441 [==============================] - 1831s 94ms/step - loss: 0.1071\n",
      "Epoch 36/50\n",
      "    7/19441 [..............................] - ETA: 32:56 - loss: 0.1229"
     ]
    }
   ],
   "source": [
    "# Define Neural Network\n",
    "neurons = 100\n",
    "model = create_model(X.shape, time_steps, neurons)\n",
    "\n",
    "# Train Model\n",
    "model.fit(X, y, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Save Neural Network\n",
    "model.save('Gap_Up_LTSM_NN.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae4bde7",
   "metadata": {},
   "source": [
    "# What I have Learned\n",
    "\n",
    "Considering this was my first LTSM NN, I think that I did a somewhat \"okay\" job Developing and getting this data for the project. However, I think that since this is my first one, I would like to point out a couple things and note to myself that I can do better, and what I need to change when I come back and Revist this project. \n",
    "\n",
    "1. In the area of Machine learning and NN development, HARDWARE IS EVERYTHING. With the advances in the GPU market, I need to upgrade my hardware. My current machine has a rtx 2070 inside. A good GPU for Minecraft, but not for NN and AI dev. I Am going to upgrade my hardware and get a machine with a 64 core CPU, 516GB of ram, and quad RTX A5000's specifcally for NN and AI Dev. \n",
    "\n",
    "2. I would like to add that after working with the model slightly, I noticed that even though the LTSM NN is able to get some info projected, after researching THERE IS NO PATTERN IN PRICE IN THE STOCK MARKET. I have started to look at other ways that I could be able to track the patterns using percents, and what other factors may have an effect on the stock market, considering stuff like, warrants, and the shelf filings, float rotations, etc. I think to get this to a state of the art project, I would need to be able to go back even further in the data, maybe like 2001-ish, and get small-market cap data from back then as well. I think that there should be many itterations of this model. I have all of the CSV's saved. Data collection and refinement should be considered, and as of right now, and LTSM models for the stock market, are not very accurate. At least the ones that are easily found. \n",
    "\n",
    "I think overall this was a very good learning experince, and I would like to come back, and deploy the model for personal use, and hopefully turn a profit. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
